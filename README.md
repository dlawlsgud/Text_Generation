# Text_Generation

### Text Summarization 
#### : 길이가 긴 본문의 의미는 유지한 채 본문보다 짧은 문장을 생성하는 자연어 처리 태스크다. 
> ##### 1. 추출 요약 : 본문에서 중요하다고 생각되는 단어, 구, 또는 문장을 그대로 추출해 문장을 생성한다.  
> ##### 2. 추상 요약 : 본문 전체 내용을 이해하고 이해한 내용을 바탕으로 새로운 문장을 생성한다.
##### - Text summarization 기본 Architecture은 Encoder에 document를 넣고 Decoder에서 summary를 출력하는 "Sequence-to-Sequence"구조다.
##### - 최근 대량 코퍼스를 사용해 Self-Supervised Learning 시킨 Pretrained-model을 이용해 Downstream task에 맞게 Fine-tuning 시키는 Transfer Learning이 좋은 성능을 보여준다.
<br><br>
### 연구주제 => Curriculum Learning을 적용한 생성요약


### Itroduction

생성요약은 추출요약보다 허위정보 생성, 잘못된 개체명 인식 등의 본문 전체 내용에 대한 이해 부족으로 비롯된 신뢰성 부분에 단점이 있다. 문법적으로 정확하고 가독성이 좋은 새로운 문장을 생성하는 능력도 중요하지만, 본문 전체 내용에 대한 이해를 바탕으로 한 중요 문장 인식 능력 또한 필요하다. 본 논문에서는 **추출요약 학습을 통해 중요 문장을 인식하는 능력을 갖춘 모델에 생성요약을 학습**시켜 기존 생성요약의 단점을 보완하고자 한다.<br> <br>
  추출 요약은 문장 선택 및 조합 문제인 반면, 생성 요약은 본문의 의미 및 담론에 대한 더 깊은 이해와 기술을 필요로 하는 어려운 태스크입니다. 해당 논문에서는 상대적으로 쉬운 과제를 먼저 학습 시키고 이후 상대적으로 어려운 과제를 학습 시키는 **커리큘럼 학습(Curriculum Learning) 을 문서 요약 태스크에 적용**해 실험을 진행했습니다.
  <br> <br>
  최근 자연어 처리에서 좋은 성능을 보이고 있는 사전학습 모델(Pre-trained Model) **BART에 본문의 중요 문장을 그대로 추출하는 과제를 통해 미세조정(Fine-tuning) 시킨 이후 추상적인 요약문을 생성하도록 미세조정 시킨 모델**과 **사전학습 모델 BART에 생성 요약 학습만 시킨 모델**의 **성능을 비교**해 **가설을 증명**했습니다.
  <br> <br>
  추출 요약문(Reference extractive-summary)이 없는 경우를 대비하여 **임시 추출 요약문(Pseudo extractive-summary)을 만들기 위해 Lead-N, TextRank, Principal 전략을 사용**했습니다. 전략을 통해 만든 임시 추출 요약문을 가지고 사전학습 모델에 커리큘럼 학습을 적용한 모델 또한 생성 요약 학습만 시킨 모델보다 뛰어난 성능을 보였습니다. 

### 학습 방법

 먼저 **총 학습 횟수를 *N* 번**으로 고정 시킨다. *추출 요약문*을 사용해 본문을 사전학습 모델에 넣었을 때 사전학습 모델이 본문 내용을 추출하여 요약 하도록 ***M* 번 학습** 시킨다. *M* 번 학습 시키는 동안 검증 데이터에서 가장 좋은 성능을 보인 모델을 저장한다. 이후 **생성 요약문**을 사용해 저장된 본문의 중요 문장 인식 능력이 가장 좋았던 모델에 본문을 넣어 본문에 없는 새로운 문장의 요약문을 만들도록 ***N-M*** 번 학습 시킨다. *N-M* 번 학습 시키는 동안 검증 데이터에서 가장 좋은 성능을 보인 모델을 통해 최종 평가 데이터에 대한 성능을 비교한다. *M*을 0부터(생성요약을 *N*번 학습) 1씩 늘려가면서 *M* : *N-M* 비율 중 어떤 모델이 가장 좋은 성능을 보이는지에 대한 실험을 진행했다. 
<br>
<br>
  만약 추출 요약문이 없는 경우 **임시 추출 요약문을 만들어 커리큘럼 학습을 진행**시켜야 한다. 본 논문에서는 임시 추출 요약문을 만드는 주요 3가지 전략을 사용했다. 
<br>
>  - **Lead** : 본문 서두 N개의 문장 선택
    <br>
>  - **TextRank** : 본문 문장들로 Graph를 만들어 중요    
    도를 계산해 top N개 문장 선택
    <br>
>  - **Principal** : 본문 각 문장과 나머지 문장들 사이의 Rouge1-F1 score를 계산해 top N개 문장 선택
     <br>
     <br>
<br> 

**Lead 전략**은 보통 **주요 내용이 본문 앞 쪽에 많이 위치**한다는 점을 고려하여 본문 서두의 문장 N개를 추출하여 임시 추출 요약문을 만들었다. **TextRank 전략**은 **PageRank 알고리즘의 영향을 받아 본문내의 각 문장들을 노드로 하고 노드 간 유사성**을 계산하여 노드를 연결함으로써 그래프를 만든다.**유사성이 높은 부분들이 서로 연결되어 각각의 문장을 Ranking**하여 순위가 높은 순서대로 N개의 문장을 추출해 임시 추출 요약문을 만든다. **Principal 전략**은 PEGASUS[6]에서 임시 추출 요약문을 만드는 알고리즘의 영향을 받아 본문 **각 문장과 그 문장을 제외한 나머지 문장들 사이의 Rouge1-F1 score를 계산**하여 순위가 높은 순서대로 N개의 문장을 추출해 임시 추출 요약문을 만들었다.

### 실험 결과
<br><br>
![1](https://user-images.githubusercontent.com/64317686/114987632-56b4ae80-9ed0-11eb-9295-27f85ca581f5.JPG)
<br>
> 위 실험을 통해 **커리큘럼 학습을 적용하여 미세조정 시킨 모델**의 성능이 단지 참조 추출요약문 혹은 추상 요약문으로만 미세조정 시킨 모델보다 **좋은 성능**을 보여줬다.
 
![1](https://user-images.githubusercontent.com/64317686/114993660-fd03b280-9ed6-11eb-92bb-f1b7286dee13.JPG)
<br>
> Lead-N 전략을 사용했을 때 생성 요약문을 가지고 미세조정 시킨 모델보다 성능이 낮았다. 하지만 **TextRank 전략**과 **Principal 전략**을 사용해 커리큘럼 학습 시킨 모델 성능은 생성 요약문을 가지고 미세조정 시킨 모델보다 **좋은 성능**을 보여줬다. TextRank 전략은 1번의 추출요약 학습 이후 9번 생성요약 학습 시켰을 때 Rouge-1, Rouge-2, Rouge-L score 모두 가장 좋은 성능을 보였다.  반면  Principal 전략을 사용 했을 때는 Rouge-1, Rouge-2, Rouge-L score 모두 각기 다른 횟수로 학습 시켰을 때 가장 좋은 성능을 보여줬다. **Rouge1-F1 score를 척도로 사용해 임시 추출 요약문을 추출한 Principal 전략이 Rouge-1 socre 부분에서 가장 좋은 성능**을 보였다. 그리고 **Rouge-2, Rouge-L score 부분에서는 Textrank 전략을 사용했을 때 성능이 가장 좋았다. **

<br><br>
### 결론
> **추출 요약문과 생성 요약문을 사용하는 커리큘럼 학습 방법이 생성요약 과제에서도 성능 향상에 도움을 준다는 가설을 증명**했다. 
추출 요약문이 없는 경우를 대비하여 임시 추출 요약문을 만드는 전략을 사용했다.  **Textrank 와 Principal 전략을 사용해 임시 추출 요약문** 을 만들어 커리큘럼 학습을 적용한 모델과 추출 요약문을 사용해 커리큘럼 학습을 진행한 모델을 비교 했을 때 **준수한 성능**을 보여줬다.
