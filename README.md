# Text_Generation

### Text Summarization 
#### : 길이가 긴 본문의 의미는 유지한 채 본문보다 짧은 문장을 생성하는 자연어 처리 태스크다. 
> ##### 1. 추출 요약 : 본문에서 중요하다고 생각되는 단어, 구, 또는 문장을 그대로 추출해 문장을 생성한다.  
> ##### 2. 추상 요약 : 본문 전체 내용을 이해하고 이해한 내용을 바탕으로 새로운 문장을 생성한다.
##### - Text summarization 기본 Architecture은 Encoder에 document를 넣고 Decoder에서 summary를 출력하는 "Sequence-to-Sequence"구조다.
##### - 최근 대량 코퍼스를 사용해 Self-Supervised Learning 시킨 Pretrained-model을 이용해 Downstream task에 맞게 Fine-tuning 시키는 Transfer Learning이 좋은 성능을 보여준다.

### 연구주제 => Curriculum Learning을 적용한 생성요약


### Itroduction

생성요약은 추출요약보다 허위정보 생성, 잘못된 개체명 인식 등의 본문 전체 내용에 대한 이해 부족으로 비롯된 신뢰성 부분에 단점이 있다. 문법적으로 정확하고 가독성이 좋은 새로운 문장을 생성하는 능력도 중요하지만, 본문 전체 내용에 대한 이해를 바탕으로 한 중요 문장 인식 능력 또한 필요하다. 본 논문에서는 **추출요약 학습을 통해 중요 문장을 인식하는 능력을 갖춘 모델에 생성요약을 학습**시켜 기존 생성요약의 단점을 보완하고자 한다.<br> <br>
  추출 요약은 문장 선택 및 조합 문제인 반면, 생성 요약은 본문의 의미 및 담론에 대한 더 깊은 이해와 기술을 필요로 하는 어려운 태스크입니다. 해당 논문에서는 상대적으로 쉬운 과제를 먼저 학습 시키고 이후 상대적으로 어려운 과제를 학습 시키는 **커리큘럼 학습(Curriculum Learning) 을 문서 요약 태스크에 적용**해 실험을 진행했습니다.
  <br> <br>
  최근 자연어 처리에서 좋은 성능을 보이고 있는 사전학습 모델(Pre-trained Model) **BART에 본문의 중요 문장을 그대로 추출하는 과제를 통해 미세조정(Fine-tuning) 시킨 이후 추상적인 요약문을 생성하도록 미세조정 시킨 모델**과 **사전학습 모델 BART에 생성 요약 학습만 시킨 모델**의 **성능을 비교**해 **가설을 증명**했습니다.
  <br> <br>
  추출 요약문(Reference extractive-summary)이 없는 경우를 대비하여 **임시 추출 요약문(Pseudo extractive-summary)을 만들기 위해 Lead-N, TextRank, Principal 전략을 사용**했습니다. 전략을 통해 만든 임시 추출 요약문을 가지고 사전학습 모델에 커리큘럼 학습을 적용한 모델 또한 생성 요약 학습만 시킨 모델보다 뛰어난 성능을 보였습니다. 


